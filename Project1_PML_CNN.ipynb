{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTJJuT5GcRml",
        "outputId": "97ba17ad-996a-46d4-9c72-95d8efa23c3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "unrar is already the newest version (1:6.1.5-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n",
            "\n",
            "UNRAR 6.11 beta 1 freeware      Copyright (c) 1993-2022 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from Project unos__.rar\n",
            "\n",
            "Extracting  test.json                                                    \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Extracting  train.json                                                   \b\b\b\b 51%\b\b\b\b 94%\b\b\b\b\b  OK \n",
            "Extracting  validation.json                                              \b\b\b\b100%\b\b\b\b\b  OK \n",
            "All OK\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)\n",
        "!apt-get install unrar\n",
        "\n",
        "# Change to the directory containing your RAR file\n",
        "!cp '/content/drive/My Drive/Project unos__.rar' './'\n",
        "\n",
        "# Unrar the file\n",
        "!unrar x 'Project unos__.rar'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cklE4R9lcW41"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "# Replace 'your_file.json' with your JSON file's name\n",
        "with open('train.json', 'r', encoding='utf8') as file:\n",
        "    train = json.load(file)\n",
        "# Replace 'your_file.json' with your JSON file's name\n",
        "with open('validation.json', 'r', encoding='utf8') as file:\n",
        "    valid = json.load(file)\n",
        "# Replace 'your_file.json' with your JSON file's name\n",
        "with open('test.json', 'r', encoding='utf8') as file:\n",
        "    test = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKggXq899C8c",
        "outputId": "3356109d-6fdb-4e08-887d-9e5c451e27fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exemplu de text cu diacritice: sarpe, mar, padure.\n"
          ]
        }
      ],
      "source": [
        "def replace_diacritics(text):\n",
        "    replacements = {'ă': 'a', 'î': 'i', 'â': 'a', 'ș': 's', 'ț': 't'}\n",
        "    for diacritic, replacement in replacements.items():\n",
        "        text = text.replace(diacritic, replacement)\n",
        "    return text\n",
        "romanian_text = \"Exemplu de text cu diacritice: șarpe, măr, pădure.\"\n",
        "processed_text = replace_diacritics(romanian_text)\n",
        "print(processed_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmHmwJc7cZuT",
        "outputId": "7f57a95c-5022-4d76-a3a9-bbc48634530b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# stopwords\n",
        "#romanian_stopwords = stopwords.words('romanian')\n",
        "\n",
        "stemmer = SnowballStemmer('romanian')\n",
        "\n",
        "def inlocuire_diacritice(propozitie):\n",
        "    inlocuire = {'ă': 'a', 'î': 'i', 'â': 'a', 'ș': 's', 'ț': 't'}\n",
        "    for diacritice, inlocuiri in inlocuire.items():\n",
        "       propozitie = propozitie.replace(diacritice, inlocuiri)\n",
        "    return propozitie\n",
        "\n",
        "\n",
        "def eliminare_structuri_html(propozitie):\n",
        "    propozitie = re.sub('<.*?>+', '', propozitie)\n",
        "    return propozitie\n",
        "\n",
        "\n",
        "def normalizare(propozitie):\n",
        "\n",
        "    #litere mici\n",
        "    propozitie = propozitie.lower()\n",
        "\n",
        "    #eliminare diacritice\n",
        "    propozitie = inlocuire_diacritice(propozitie)\n",
        "\n",
        "    #eliminare html\n",
        "    propozitie = eliminare_structuri_html(propozitie)\n",
        "\n",
        "    #elimminare semne de punctuatie\n",
        "    propozitie = re.sub(r'[^\\w\\s]', '', propozitie)\n",
        "\n",
        "    #Tokenizare\n",
        "    tokens = nltk.word_tokenize(propozitie)\n",
        "\n",
        "    # stemming\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    # contopirea tokenilor stemmed\n",
        "    propozitie = ' '.join(stemmed_tokens)\n",
        "\n",
        "    return propozitie\n",
        "\n",
        "def preprocesare(data,nolabel = False):\n",
        "    labels = []\n",
        "    guid = []\n",
        "    propozitie1 = []\n",
        "    propozitie2 = []\n",
        "    for inregistrare in data:\n",
        "        propozitie1.append(normalizare(inregistrare[\"sentence1\"]))\n",
        "        propozitie2.append(normalizare(inregistrare[\"sentence2\"]))\n",
        "\n",
        "        if nolabel != True:\n",
        "            labels.append(inregistrare[\"label\"])\n",
        "\n",
        "        guid.append(inregistrare[\"guid\"])\n",
        "\n",
        "    return (propozitie1,propozitie2),labels,guid\n",
        "\n",
        "\n",
        "preprocessed_train = preprocesare(train)\n",
        "preprocessed_valid = preprocesare(valid)\n",
        "preprocessed_test = preprocesare(test,nolabel = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nINhYf56xfc"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "rwzaZbTaLsjW",
        "outputId": "99a48303-b18b-4a4d-c85d-3fa62c31bde1"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'preprocessed_train' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-17e224c2d46d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# definiea etichetelor pentru train si validare\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpreprocessed_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlabels_one_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#exista 4 clase, realizam one hot encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlabels_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessed_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlabels_valid_one_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'preprocessed_train' is not defined"
          ]
        }
      ],
      "source": [
        "# definiea etichetelor pentru train si validare\n",
        "labels =  preprocessed_train[1]\n",
        "labels_one_hot = to_categorical(labels, num_classes=4) #exista 4 clase, realizam one hot encoding\n",
        "labels_valid = preprocessed_valid[1]\n",
        "labels_valid_one_hot = to_categorical(labels_valid, num_classes=4)\n",
        "\n",
        "# tokenzirea propozitiilor\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "propozitie1, propozitie2 = preprocessed_train[0]\n",
        "propozitie1_valid, propozitie2_valid = preprocessed_valid[0]\n",
        "\n",
        "# fit pe text, ofera un vocabular cu toate cuvintele unice\n",
        "tokenizer.fit_on_texts(propozitie1 + propozitie2)\n",
        "\n",
        "#pentru datele din antrenare\n",
        "secventa1 = tokenizer.texts_to_sequences(propozitie1)\n",
        "secventa2 = tokenizer.texts_to_sequences(propozitie2)\n",
        "\n",
        "#pentru datele din validare\n",
        "secventa1_valid = tokenizer.texts_to_sequences(propozitie1_valid)\n",
        "secventa2_valid = tokenizer.texts_to_sequences(propozitie2_valid)\n",
        "\n",
        "\n",
        "#aflarea lungimii maxime pentru padding\n",
        "lungime_maxima = max(max(len(s) for s in secventa1), max(len(s) for s in secventa2))\n",
        "\n",
        "# aplicarea padding ului pentru a avea secvente de aceasi lungime\n",
        "secventa1_padded = pad_sequences(secventa1, maxlen=lungime_maxima , padding='post')\n",
        "secventa2_padded = pad_sequences(secventa2, maxlen=lungime_maxima , padding='post')\n",
        "\n",
        "secventa1_padded_valid = pad_sequences(secventa1_valid, maxlen=lungime_maxima , padding='post')\n",
        "secventa2_padded_valid = pad_sequences(secventa2_valid, maxlen=lungime_maxima , padding='post')\n",
        "\n",
        "#combinarea propozitiile pentru construirea vocabularului\n",
        "toate_propozitiile = preprocessed_train[0][0] + preprocessed_train[0][1]\n",
        "#crearea unui set care va reprezenta vocabularul, fiind un set, cuvintele sunt unice, neavand duplicate\n",
        "vocabular = set(cuvant for propozitie in toate_propozitiile for cuvant in propozitie.split())\n",
        "dimensiune_vocabular = len(vocabular)\n",
        "\n",
        "# Definirea parametrilor modelului\n",
        "dimensiune_vocabular = len(tokenizer.word_index) + 1\n",
        "dimensiune_embedding = 100\n",
        "lungime_maxima = max(max(len(s) for s in secventa1), max(len(s) for s in secventa2)) #calcularea lungimii secventei maxime\n",
        "numar_clase = 4 #cele 4 clase\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OP1YMsOSeNk8"
      },
      "outputs": [],
      "source": [
        "def f1_score(y_true, y_pred):\n",
        "\n",
        "    # Calcularea preciziei si recall ului pentru fiecare clasa\n",
        "    TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)), axis=0) #prezicerile corecte pe toate clasele\n",
        "    total_preziceri = K.sum(K.round(K.clip(y_true, 0, 1)), axis=0) #numarul total de preziceri corecte si incorecte\n",
        "    predictii_adevarate = K.sum(K.round(K.clip(y_pred, 0, 1)), axis=0) #calcularea numarului total de preziceri corecte de catre model\n",
        "\n",
        "    precision = TP / (predictii_adevarate + K.epsilon())\n",
        "    recall = TP / (total_preziceri + K.epsilon())\n",
        "\n",
        "    # Calcularea f1 score pt fiecare clasa\n",
        "    f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
        "\n",
        "    # Calcularea macro F1 score pe toate clasele\n",
        "    macro_f1 = K.mean(f1)\n",
        "    return macro_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCmVq8zO0fjE",
        "outputId": "aa2225e1-7add-4d61-919b-c2c4e421a47f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "228/228 [==============================] - 34s 140ms/step - loss: 1.3871 - accuracy: 0.2612 - f1_score: 0.0000e+00 - val_loss: 1.3953 - val_accuracy: 0.1353 - val_f1_score: 0.0000e+00\n",
            "Epoch 2/30\n",
            "228/228 [==============================] - 26s 115ms/step - loss: 1.3625 - accuracy: 0.3610 - f1_score: 9.3199e-04 - val_loss: 1.3024 - val_accuracy: 0.4227 - val_f1_score: 0.0014\n",
            "Epoch 3/30\n",
            "228/228 [==============================] - 24s 104ms/step - loss: 1.2141 - accuracy: 0.4285 - f1_score: 0.1118 - val_loss: 1.0788 - val_accuracy: 0.5011 - val_f1_score: 0.2002\n",
            "Epoch 4/30\n",
            "228/228 [==============================] - 19s 85ms/step - loss: 1.0318 - accuracy: 0.5001 - f1_score: 0.2695 - val_loss: 1.1238 - val_accuracy: 0.4861 - val_f1_score: 0.2295\n",
            "Epoch 5/30\n",
            "228/228 [==============================] - 20s 88ms/step - loss: 0.8771 - accuracy: 0.5649 - f1_score: 0.3806 - val_loss: 1.0382 - val_accuracy: 0.5260 - val_f1_score: 0.2771\n",
            "Epoch 6/30\n",
            "228/228 [==============================] - 20s 86ms/step - loss: 0.7385 - accuracy: 0.6167 - f1_score: 0.4796 - val_loss: 1.1026 - val_accuracy: 0.5077 - val_f1_score: 0.3022\n",
            "Epoch 7/30\n",
            "228/228 [==============================] - 19s 82ms/step - loss: 0.6296 - accuracy: 0.6613 - f1_score: 0.5622 - val_loss: 0.9663 - val_accuracy: 0.5652 - val_f1_score: 0.3167\n",
            "Epoch 8/30\n",
            "228/228 [==============================] - 17s 74ms/step - loss: 0.5357 - accuracy: 0.6994 - f1_score: 0.6270 - val_loss: 1.0070 - val_accuracy: 0.5704 - val_f1_score: 0.3507\n",
            "Epoch 9/30\n",
            "228/228 [==============================] - 18s 78ms/step - loss: 0.4490 - accuracy: 0.7457 - f1_score: 0.6981 - val_loss: 1.0595 - val_accuracy: 0.5623 - val_f1_score: 0.3592\n",
            "Epoch 10/30\n",
            "228/228 [==============================] - 17s 77ms/step - loss: 0.3840 - accuracy: 0.7818 - f1_score: 0.7465 - val_loss: 1.0384 - val_accuracy: 0.5904 - val_f1_score: 0.3662\n",
            "Epoch 11/30\n",
            "228/228 [==============================] - 18s 78ms/step - loss: 0.3242 - accuracy: 0.8135 - f1_score: 0.7908 - val_loss: 1.0953 - val_accuracy: 0.5842 - val_f1_score: 0.3692\n",
            "Epoch 12/30\n",
            "228/228 [==============================] - 17s 75ms/step - loss: 0.2779 - accuracy: 0.8424 - f1_score: 0.8260 - val_loss: 1.1689 - val_accuracy: 0.5793 - val_f1_score: 0.3663\n",
            "Epoch 13/30\n",
            "228/228 [==============================] - 17s 73ms/step - loss: 0.2349 - accuracy: 0.8711 - f1_score: 0.8604 - val_loss: 1.1464 - val_accuracy: 0.6048 - val_f1_score: 0.3706\n",
            "Epoch 14/30\n",
            "228/228 [==============================] - 16s 72ms/step - loss: 0.2020 - accuracy: 0.8898 - f1_score: 0.8820 - val_loss: 1.3010 - val_accuracy: 0.5806 - val_f1_score: 0.3714\n",
            "Epoch 15/30\n",
            "228/228 [==============================] - 17s 75ms/step - loss: 0.1725 - accuracy: 0.9085 - f1_score: 0.9018 - val_loss: 1.3100 - val_accuracy: 0.6008 - val_f1_score: 0.3718\n",
            "Epoch 16/30\n",
            "228/228 [==============================] - 16s 72ms/step - loss: 0.1445 - accuracy: 0.9242 - f1_score: 0.9180 - val_loss: 1.3942 - val_accuracy: 0.5953 - val_f1_score: 0.3694\n",
            "Epoch 17/30\n",
            "228/228 [==============================] - 17s 76ms/step - loss: 0.1252 - accuracy: 0.9379 - f1_score: 0.9345 - val_loss: 1.5437 - val_accuracy: 0.5783 - val_f1_score: 0.3606\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78af500cf4f0>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Concatenate, Flatten, Dropout, Conv1D, MaxPooling1D\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "#Crearea arhitecturii modelului\n",
        "input1 = Input(shape=(lungime_maxima,))\n",
        "input2 = Input(shape=(lungime_maxima,))\n",
        "\n",
        "embedding_layer = Embedding(dimensiune_vocabular, dimensiune_embedding, input_length=lungime_maxima)\n",
        "embedded1 = embedding_layer(input1)\n",
        "embedded2 = embedding_layer(input2)\n",
        "\n",
        "concatenated = Concatenate(axis=1)([embedded1, embedded2]) #concatenarea celor 2 straturi embedded\n",
        "\n",
        "conv1 = Conv1D(64, 2, activation='relu')(concatenated) #cele 2 straturi Conv1d aplica filtre care invata anumite pattern uri\n",
        "pool1 = MaxPooling1D(2)(conv1) #reduce dimensionalitatea pentru a putea face presupuneri\n",
        "conv2 = Conv1D(32, 2, activation='relu')(pool1)\n",
        "flat = Flatten()(conv2) #se realizeaza un singur vector\n",
        "dense1 = Dense(200, activation='relu')(flat) #un strat full conectat care proceseaza caracteristici extrase din straturile convolutionale\n",
        "drop1 = Dropout(0.5)(dense1)\n",
        "output = Dense(numar_clase, activation='softmax')(drop1) #4 neuroni, care reprezinta fiecare clasa cu o probabilitate data de functia softmax\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=[input1, input2], outputs=output)\n",
        "\n",
        "# early stopping\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10, #verifica validation loss si se va opri dupa 5 epoci daca nu sunt progrese\n",
        "    restore_best_weights=True #se salveaza cele mai bune weights uri\n",
        ")\n",
        "\n",
        "# ModelCheckpoint callback\n",
        "checkpoint = ModelCheckpoint('my_model.keras', save_best_only=True, monitor='val_loss', mode='min')\n",
        "\n",
        "# Calcularea class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels) #se creaza weights uri pentru fiecare clasa in vederea dataset ului care nu este echilibrat\n",
        "class_weights_dict = {i : weight for i, weight in enumerate(class_weights)}\n",
        "\n",
        "\n",
        "#optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "#Compilarea modelului\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy', f1_score])\n",
        "\n",
        "# Antrenarea\n",
        "model.fit([secventa1_padded, secventa2_padded], labels_one_hot,\n",
        "          validation_data=([secventa1_padded_valid, secventa2_padded_valid], labels_valid_one_hot),\n",
        "          epochs=30,\n",
        "          batch_size=256,\n",
        "          class_weight=class_weights_dict,\n",
        "          callbacks=[early_stopping,checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-531A5lTEM63",
        "outputId": "5fcefd9a-8c63-405e-967b-ae2b53e2d26e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "94/94 [==============================] - 3s 28ms/step\n",
            "(3000, 1)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "def f1_score(y_true, y_pred):\n",
        "\n",
        "    # Calcularea preciziei si recall ului pentru fiecare clasa\n",
        "    TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)), axis=0) #prezicerile corecte pe toate clasele\n",
        "    total_preziceri = K.sum(K.round(K.clip(y_true, 0, 1)), axis=0) #numarul total de preziceri corecte si incorecte\n",
        "    predictii_adevarate = K.sum(K.round(K.clip(y_pred, 0, 1)), axis=0) #calcularea numarului total de preziceri corecte de catre model\n",
        "\n",
        "    precision = TP / (predictii_adevarate + K.epsilon())\n",
        "    recall = TP / (total_preziceri + K.epsilon())\n",
        "\n",
        "    # Calcularea f1 score pt fiecare clasa\n",
        "    f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
        "\n",
        "    # Calcularea macro F1 score pe toate clasele\n",
        "    macro_f1 = K.mean(f1)\n",
        "    return macro_f1\n",
        "\n",
        "#realizam acelasi procedeu la fel ca la preprocesarea setului de antrenare si validare, de data aceasta pentru cel de testare\n",
        "propozitia1_test, propozitia2_test = preprocessed_test[0]\n",
        "\n",
        "propozitia1_test = tokenizer.texts_to_sequences(propozitia1_test)\n",
        "propozitia2_test = tokenizer.texts_to_sequences(propozitia2_test)\n",
        "\n",
        "propozitia1_padded_test = pad_sequences(propozitia1_test, maxlen=lungime_maxima, padding='post')\n",
        "propozitia2_padded_test = pad_sequences(propozitia2_test, maxlen=lungime_maxima, padding='post')\n",
        "\n",
        "#preluam cele mai bune weights uri\n",
        "best_model = load_model('my_model.keras',custom_objects={'f1_score': f1_score})\n",
        "#realizam predictia\n",
        "predicted_labels = best_model.predict((propozitia1_padded_test,propozitia2_padded_test))\n",
        "\n",
        "predicted_labels = np.argmax(predicted_labels, axis=1)\n",
        "preprocessed_test_array = np.array(preprocessed_test[2]).reshape((-1, 1))\n",
        "predicted_labels_array = np.array(predicted_labels).reshape((-1, 1))\n",
        "\n",
        "import pandas as pd\n",
        "#crearea csv\n",
        "print(predicted_labels_array.shape)\n",
        "\n",
        "data = np.hstack([preprocessed_test_array, predicted_labels_array])\n",
        "df = pd.DataFrame(data, columns=[\"guid\", \"label\"])\n",
        "\n",
        "df.to_csv('CNN2.csv',index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
